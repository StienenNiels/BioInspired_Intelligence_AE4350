{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "from stable_baselines3 import SAC\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv, SubprocVecEnv, VecMonitor, VecNormalize\n",
    "from stable_baselines3.common.vec_env.base_vec_env import VecEnv, VecEnvStepReturn, VecEnvWrapper\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VecPendulumRewardWrapper(VecEnvWrapper):\n",
    "    def __init__(self, venv: VecEnv):\n",
    "        super().__init__(venv=venv)\n",
    "\n",
    "    def reset(self) -> np.ndarray:\n",
    "        obs = self.venv.reset()\n",
    "        return obs\n",
    "    \n",
    "    def step_async(self, actions: np.ndarray) -> None:\n",
    "        self.venv.step_async(actions)\n",
    "\n",
    "    def step_wait(self) -> VecEnvStepReturn:\n",
    "        obs, reward, done, info = self.venv.step_wait()\n",
    "        reward = 1/obs[:, 0]**2 + 1/obs[:, 1]**2\n",
    "        return obs, reward, done, info\n",
    "\n",
    "def make_env(env_id, rank, seed=0):\n",
    "    def _init():\n",
    "        env = gym.make(env_id)\n",
    "        return env\n",
    "    return _init\n",
    "\n",
    "env_id = 'InvertedPendulum-v4'  # Replace with your MuJoCo environment\n",
    "num_envs = 16  # Number of parallel environments\n",
    "\n",
    "# Create the vectorized environment\n",
    "env = SubprocVecEnv([make_env(env_id, i) for i in range(num_envs)])\n",
    "print(env.seed())\n",
    "# Add the reward wrapper\n",
    "env = VecPendulumRewardWrapper(env)\n",
    "env = VecMonitor(env)  # Optional: for monitoring and logging\n",
    "\n",
    "model = SAC('MlpPolicy', env, verbose=0)\n",
    "model.learn(total_timesteps=1e5, log_interval=4, progress_bar=True)\n",
    "model.save(\"sac_pendulum\")\n",
    "\n",
    "# del model # remove to demonstrate saving and loading\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SAC.load(\"sac_pendulum\")\n",
    "\n",
    "class PendulumRewardWrapper(gym.Wrapper):\n",
    "    def __init__(self, env):\n",
    "        super().__init__(env=env)\n",
    "\n",
    "    def reset(self):\n",
    "        obs = self.env.reset()\n",
    "        return obs\n",
    "\n",
    "    def step(self, action):\n",
    "        obs, reward, terminated, truncated, info = self.env.step(action)\n",
    "        reward = 1/obs[0]**2 + 1/obs[1]**2\n",
    "        return obs, reward, terminated, truncated, info\n",
    "\n",
    "env = gym.make(\"InvertedPendulum-v4\", render_mode='rgb_array')\n",
    "env = PendulumRewardWrapper(env)\n",
    "obs, info = env.reset()\n",
    "\n",
    "data = []\n",
    "total_reward = 0\n",
    "terminated = False\n",
    "truncated = False\n",
    "# while not (terminated or truncated):\n",
    "while not truncated:\n",
    "    action, _states = model.predict(obs, deterministic=True)\n",
    "    obs, reward, terminated, truncated, info = env.step(action)\n",
    "    total_reward += reward\n",
    "    data.append(np.concatenate((action, obs, np.array([total_reward]))))\n",
    "    # clear_output(wait=True)\n",
    "    # plt.imshow( env.render() )\n",
    "    # plt.axis('off')\n",
    "    # plt.show()\n",
    "\n",
    "names = [\"action\", \"cart_pos\", \"pole_angle\", \"cart_velocity\", \"pole_ang_vel\", \"reward\"]\n",
    "\n",
    "P = pd.DataFrame(data, columns = names)\n",
    "\n",
    "name = \"testrun\"\n",
    "saveFile = \"recordings/\" + name\n",
    "P.to_csv(saveFile + \".csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs, info = env.reset()\n",
    "while True:\n",
    "    action, _states = model.predict(obs, deterministic=True)\n",
    "    obs, reward, terminated, truncated, info = env.step(action)\n",
    "    if terminated or truncated:\n",
    "        obs, info = env.reset()\n",
    "    clear_output(wait=True)\n",
    "    plt.imshow( env.render() )\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "BII_mujoco",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
